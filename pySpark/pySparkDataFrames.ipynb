{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What we will learn in this notebook \n",
    "* how to deal with pyspark Dataframes \n",
    "* reading the dataset \n",
    "* checking the data types of the columns, and the schema \n",
    "* selecting columns and indexing\n",
    "* check and describe the option similar to pandas. \n",
    "* adding columns\n",
    "* dropping columns\n",
    "* Dropping rows \n",
    "* Various parameter in dropping functionalities\n",
    "* handling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyspark and pandas \n",
    "import pandas as pd \n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to deal with pyspark Dataframes & Reading the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a session for spark \n",
    "spark = SparkSession.builder.appName('DataFrame').getOrCreate() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://AbdelazizPC:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>example1</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x12c417fd040>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the dataset \n",
    "df_pySpark = spark.read.option('header', 'true').csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+\n",
      "|     Name|Age|Experience|\n",
      "+---------+---+----------+\n",
      "|    Ahmed| 27|        10|\n",
      "|Abdelaziz| 23|        20|\n",
      "|     Eman| 30|         5|\n",
      "+---------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to show the rows, we use the show  method\n",
    "df_pySpark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Checking the datatypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Experience: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we can print the schema \n",
    "df_pySpark.printSchema() \n",
    "# below you can notice that everything is of type string, but age and experience are of type int, so how to make them int? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+\n",
      "|     Name|Age|Experience|\n",
      "+---------+---+----------+\n",
      "|    Ahmed| 27|        10|\n",
      "|Abdelaziz| 23|        20|\n",
      "|     Eman| 30|         5|\n",
      "+---------+---+----------+\n",
      "\n",
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# there is a property called inferSchema, we should set it to true, to make spark automatically infer the datatype \n",
    "df_pySpark = spark.read.option('header', 'true').csv('data.csv', inferSchema = True)\n",
    "df_pySpark.show()\n",
    "df_pySpark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+\n",
      "|     Name|Age|Experience|\n",
      "+---------+---+----------+\n",
      "|    Ahmed| 27|        10|\n",
      "|Abdelaziz| 23|        20|\n",
      "|     Eman| 30|         5|\n",
      "+---------+---+----------+\n",
      "\n",
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    " # we can read it also in another way, in one line\n",
    "df_pySpark = spark.read.csv('data.csv', inferSchema = True, header= True)\n",
    "df_pySpark.show()\n",
    "df_pySpark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Check describe option similar to pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name', 'Age', 'Experience']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## this is how to get all the columns in the dataframe -> similar to pandas\n",
    "df_pySpark.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Name='Ahmed', Age=27, Experience=10),\n",
       " Row(Name='Abdelaziz', Age=23, Experience=20),\n",
       " Row(Name='Eman', Age=30, Experience=5)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the top 3 rows\n",
    "df_pySpark.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     Name|\n",
      "+---------+\n",
      "|    Ahmed|\n",
      "|Abdelaziz|\n",
      "|     Eman|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To select a column, we use the select method \n",
    "df_pySpark.select('Name').show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+\n",
      "|     Name|Age|\n",
      "+---------+---+\n",
      "|    Ahmed| 27|\n",
      "|Abdelaziz| 23|\n",
      "|     Eman| 30|\n",
      "+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# multiple cols\n",
    "df_pySpark.select(['Name', 'Age']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that we can not use the same method in pandas by accessing the column using the square brackets [...] -> this will return object of Column, and not all rows in the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'Name'>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pySpark['Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Name', 'string'), ('Age', 'int'), ('Experience', 'int')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to get all the datatypes \n",
    "df_pySpark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------------------+------------------+\n",
      "|summary|     Name|               Age|        Experience|\n",
      "+-------+---------+------------------+------------------+\n",
      "|  count|        3|                 3|                 3|\n",
      "|   mean|     NULL|26.666666666666668|11.666666666666666|\n",
      "| stddev|     NULL| 3.511884584284246| 7.637626158259733|\n",
      "|    min|Abdelaziz|                23|                 5|\n",
      "|    max|     Eman|                30|                20|\n",
      "+-------+---------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pySpark.describe().show() # statistical operation like in pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Adding and Dropping columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------------------------+\n",
      "|     Name|Age|Experience|Experience After 2 Years|\n",
      "+---------+---+----------+------------------------+\n",
      "|    Ahmed| 27|        10|                      12|\n",
      "|Abdelaziz| 23|        20|                      22|\n",
      "|     Eman| 30|         5|                       7|\n",
      "+---------+---+----------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# withColumn, returns a new dataframe, by adding a new column, or replacing an existing one.\n",
    "newDf = df_pySpark.withColumn('Experience After 2 Years', df_pySpark['Experience'] + 2, )\n",
    "newDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+\n",
      "|     Name|Age|\n",
      "+---------+---+\n",
      "|    Ahmed| 27|\n",
      "|Abdelaziz| 23|\n",
      "|     Eman| 30|\n",
      "+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dropping columns \n",
    "droped = df_pySpark.drop('Experience')\n",
    "droped.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+\n",
      "| New Name|Age|Experience|\n",
      "+---------+---+----------+\n",
      "|    Ahmed| 27|        10|\n",
      "|Abdelaziz| 23|        20|\n",
      "|     Eman| 30|         5|\n",
      "+---------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    " # renaming columns \n",
    "df_pySpark.withColumnRenamed('Name', 'New Name').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Dropping rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+----------+-------+\n",
      "|     Name|  Age|Experience| Salary|\n",
      "+---------+-----+----------+-------+\n",
      "|    Ahmed| 27.0|      10.0|30000.0|\n",
      "|Abdelaziz| 23.0|      20.0|25000.0|\n",
      "|     Eman| 32.0|      50.0|10000.0|\n",
      "|     Kmal| NULL|      NULL|13000.0|\n",
      "|     NULL| NULL|      NULL|   NULL|\n",
      "|     Omar|350.0|       1.0|   NULL|\n",
      "|  hamdoly| 20.0|      15.0|   NULL|\n",
      "+---------+-----+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_data = spark.read.csv('data.csv', header= True, inferSchema=True)\n",
    "new_data.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+-------+\n",
      "|     Name| Age|Experience| Salary|\n",
      "+---------+----+----------+-------+\n",
      "|    Ahmed|27.0|      10.0|30000.0|\n",
      "|Abdelaziz|23.0|      20.0|25000.0|\n",
      "|     Eman|32.0|      50.0|10000.0|\n",
      "+---------+----+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# in order to get the null rows, we use the .na property, then on them, we can drop any row which contains null\n",
    "new_data.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+----------+-------+\n",
      "|     Name|  Age|Experience| Salary|\n",
      "+---------+-----+----------+-------+\n",
      "|    Ahmed| 27.0|      10.0|30000.0|\n",
      "|Abdelaziz| 23.0|      20.0|25000.0|\n",
      "|     Eman| 32.0|      50.0|10000.0|\n",
      "|     Kmal| NULL|      NULL|13000.0|\n",
      "|     Omar|350.0|       1.0|   NULL|\n",
      "|  hamdoly| 20.0|      15.0|   NULL|\n",
      "+---------+-----+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    in drop\n",
    "        1. how = any -> this means that if we found any null value, we should drop it\n",
    "        2. how = all -> this means that the entire row should be null in order to drop it.\n",
    "        2. how = all -> this means that the entire row should be null in order to drop it.\n",
    "\n",
    "\n",
    "'''\n",
    "new_data.na.drop(how = 'all').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+----------+-------+\n",
      "|     Name|  Age|Experience| Salary|\n",
      "+---------+-----+----------+-------+\n",
      "|    Ahmed| 27.0|      10.0|30000.0|\n",
      "|Abdelaziz| 23.0|      20.0|25000.0|\n",
      "|     Eman| 32.0|      50.0|10000.0|\n",
      "|     Omar|350.0|       1.0|   NULL|\n",
      "|  hamdoly| 20.0|      15.0|   NULL|\n",
      "+---------+-----+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Threshold: \n",
    "        it indicates that atleast Threshold # of elements must be non null, otherwise drop this row\n",
    "        this mean if threshold was 3 and we have a row which consist of 4 columns, and 2 or more were nulls, it will be droped.\n",
    "\n",
    "'''\n",
    "new_data.na.drop(thresh= 3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+----------+-------+\n",
      "|     Name|  Age|Experience| Salary|\n",
      "+---------+-----+----------+-------+\n",
      "|    Ahmed| 27.0|      10.0|30000.0|\n",
      "|Abdelaziz| 23.0|      20.0|25000.0|\n",
      "|     Eman| 32.0|      50.0|10000.0|\n",
      "|     Kmal| NULL|      NULL|13000.0|\n",
      "|     Omar|350.0|       1.0|   NULL|\n",
      "|  hamdoly| 20.0|      15.0|   NULL|\n",
      "+---------+-----+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Subset:\n",
    "        We check on the null values of certain columns, if there are nulls there, then we can drop, otherwise we will keep them\n",
    "'''\n",
    "new_data.na.drop(subset= ['Name']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Filling the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+----------+-------+\n",
      "|         Name|  Age|Experience| Salary|\n",
      "+-------------+-----+----------+-------+\n",
      "|        Ahmed| 27.0|      10.0|30000.0|\n",
      "|    Abdelaziz| 23.0|      20.0|25000.0|\n",
      "|         Eman| 32.0|      50.0|10000.0|\n",
      "|         Kmal| NULL|      NULL|13000.0|\n",
      "|Missing Value| NULL|      NULL|   NULL|\n",
      "|         Omar|350.0|       1.0|   NULL|\n",
      "|      hamdoly| 20.0|      15.0|   NULL|\n",
      "+-------------+-----+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    To fill the missing values, we first need to access the null elements, then use the fill method\n",
    "        it takes the filling value as first parameter\n",
    "        the columns in which we want to fill \n",
    "\n",
    "'''\n",
    "new_data.na.fill('Missing Value', 'Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+----------+-------+\n",
      "|     Name|  Age|Experience| Salary|\n",
      "+---------+-----+----------+-------+\n",
      "|    Ahmed| 27.0|      10.0|30000.0|\n",
      "|Abdelaziz| 23.0|      20.0|25000.0|\n",
      "|     Eman| 32.0|      50.0|10000.0|\n",
      "|     Kmal| NULL|      NULL|13000.0|\n",
      "|     miss| NULL|      NULL|   NULL|\n",
      "|     Omar|350.0|       1.0|   NULL|\n",
      "|  hamdoly| 20.0|      15.0|   NULL|\n",
      "+---------+-----+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_data.na.fill({'Name': 'miss'}).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+----------+-------+\n",
      "|     Name|  Age|Experience| Salary|\n",
      "+---------+-----+----------+-------+\n",
      "|    Ahmed| 27.0|      10.0|30000.0|\n",
      "|Abdelaziz| 23.0|      20.0|25000.0|\n",
      "|     Eman| 32.0|      50.0|10000.0|\n",
      "|     Kmal| NULL|      NULL|13000.0|\n",
      "|     NULL| NULL|      NULL|   NULL|\n",
      "|     Omar|350.0|       1.0|   NULL|\n",
      "|  hamdoly| 20.0|      15.0|   NULL|\n",
      "+---------+-----+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
