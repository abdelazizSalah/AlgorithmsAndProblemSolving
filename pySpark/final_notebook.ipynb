{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyspark \n",
    "from pyspark import SparkContext\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a session \n",
    "session = SparkContext('local', 'Exam')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing to generate the RDD in the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# read the data \n",
    "# data = pd.read_csv('final_exam_data.csv')\n",
    "# data.head()\n",
    "data = session.textFile('final_exam_data.csv')\n",
    "data.collect()\n",
    "\n",
    "# extracting the header \n",
    "header = data.first()\n",
    "\n",
    "# getting the other rows \n",
    "data_rdd = data.filter(lambda row: row != header).map(lambda line: line.split(\",\"))\n",
    "\n",
    "data = data_rdd.collect()\n",
    "\n",
    "# converting the last columns to int\n",
    "for counter in range(len(data)):\n",
    "    data[counter][0] = int(data[counter][0])\n",
    "    data[counter][3] = int(data[counter][3])\n",
    "    data[counter][4] = int(data[counter][4])\n",
    "data_rdd = session.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 'John', 'HR', 35, 50000],\n",
       " [2, 'Jane', 'IT', 28, 60000],\n",
       " [3, 'Bob', 'IT', 30, 75000],\n",
       " [4, 'Alice', 'Hr', 40, 55000],\n",
       " [5, 'Charlie', 'Finance', 45, 80000]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_rdd.collect()\n",
    "# now we can start solving our problem :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Print the names of the employees that are older than 40\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, None, None, None, 'Charlie']\n"
     ]
    }
   ],
   "source": [
    "def get_older_than_40(item): \n",
    "    if item[3] > 40:\n",
    "        return (item[1])\n",
    "\n",
    "name_and_age_rdd = data_rdd.map(get_older_than_40)\n",
    "print(name_and_age_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Employees older than 40: ['Charlie']\n"
     ]
    }
   ],
   "source": [
    "# this is a better solution, because we want to select some employees with certain tasks.\n",
    "older_than_40 = data_rdd.filter(lambda row: row[3] > 40)\n",
    "older_names = older_than_40.map(lambda row: row[1]).collect()\n",
    "print(\"Employees older than 40:\", older_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Print the sum of salaries in the IT department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('HR', 50000), ('IT', 60000), ('IT', 75000), ('Hr', 55000), ('Finance', 80000)]\n",
      "[('HR', 50000), ('IT', 135000), ('Hr', 55000), ('Finance', 80000)]\n",
      "('IT', 135000)\n"
     ]
    }
   ],
   "source": [
    "# first we need to group them by the department\n",
    "# 1. make a list of pairs, the key is the name, and the value is the age using the map\n",
    "def get_departments_and_salary(item): \n",
    "    return (item[2], item[4])\n",
    "\n",
    "dept_and_salaries = data_rdd.map(get_departments_and_salary)\n",
    "print(dept_and_salaries.collect())\n",
    "\n",
    "# then apply reduce by key \n",
    "salaries = dept_and_salaries.reduceByKey(lambda x,y: x + y)\n",
    "print(salaries.collect())\n",
    "\n",
    "# get the IT \n",
    "for item in salaries.collect():\n",
    "    if item[0] == 'IT':\n",
    "        print(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total IT department salary: 135000\n"
     ]
    }
   ],
   "source": [
    "it_salaries = data_rdd.filter(lambda row: row[2] == \"IT\").map(lambda row: int(row[4]))\n",
    "it_total_salary = it_salaries.reduce(lambda x, y: x + y) \n",
    "print(\"Total IT department salary:\", it_total_salary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[60000, 75000]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "it_sals = data_rdd.filter(lambda row: row[2] == 'IT')\n",
    "it_sals.collect()\n",
    "\n",
    "it_sals.map(lambda x: x[4]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Given a dataframe containing the employees data named EmpDF write PySpark code using DFs only to solve this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing sparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# creating a session \n",
    "spark = SparkSession.builder.appName('Exam').getOrCreate()\n",
    "\n",
    "# reading the csv in dataframe \n",
    "'''\n",
    "    inferSchema to know the datatype automatically, otherwise it will be read as string.\n",
    "    header to make the first row as the header, otherwise it will consider it as features.\n",
    "'''\n",
    "employees_df = spark.read.csv('final_exam_data.csv', inferSchema = True, header = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Index=1, name='John', Department='HR', Age=35, Salary=50000),\n",
       " Row(Index=2, name='Jane', Department='IT', Age=28, Salary=60000),\n",
       " Row(Index=3, name='Bob', Department='IT', Age=30, Salary=75000),\n",
       " Row(Index=4, name='Alice', Department='Hr', Age=40, Salary=55000),\n",
       " Row(Index=5, name='Charlie', Department='Finance', Age=45, Salary=80000)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employees_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can start solving the problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Calculate the average salary of employees in each department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|Department|avg(Salary)|\n",
      "+----------+-----------+\n",
      "|        HR|    50000.0|\n",
      "|   Finance|    80000.0|\n",
      "|        Hr|    55000.0|\n",
      "|        IT|    67500.0|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# do not forget to import the functions \n",
    "from pyspark.sql.functions import avg\n",
    "# 1. we need to group the data by department first \n",
    "department_df = employees_df.groupBy('Department')\n",
    "department_df.agg(avg(\"Salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|Department|avg(Salary)|\n",
      "+----------+-----------+\n",
      "|        HR|    50000.0|\n",
      "|   Finance|    80000.0|\n",
      "|        Hr|    55000.0|\n",
      "|        IT|    67500.0|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avg_salary_df = employees_df.select(\"Department\", \"Salary\").groupBy(\"Department\").agg(avg(\"Salary\"))\n",
    "avg_salary_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the name of the employee with the highest salary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to select the max \n",
    "from pyspark.sql.functions import max\n",
    "maxSalary = employees_df.select(max(\"Salary\")).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80000"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxSalary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Charlie'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we can filter based on this salary \n",
    "names = employees_df.filter(employees_df['Salary'] == maxSalary)\n",
    "name = names.collect()[0][1]\n",
    "name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
