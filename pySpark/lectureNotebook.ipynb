{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyspark \n",
    "import pyspark as ps\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a spark context \n",
    "# sc = SparkContext ('local', 'Lecture_Example') # local must be written small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating RDDs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of items \n",
    "data = [('emp1', 2000),('emp2', 2500),('emp3', 3000)]\n",
    "rdd = sc.parallelize(data) # this is how to create a RDD from data. \n",
    "\n",
    "# also we can create a text file and read our data from  using the textFile method \n",
    "data_file = sc.textFile('data.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### printing RDDs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('emp1', 2000), ('emp2', 2500), ('emp3', 3000)]\n",
      "['emp1, 2000', 'emp2, 2500', 'emp3, 3000']\n"
     ]
    }
   ],
   "source": [
    "# to get all the elements in the RDD, we use the method colect \n",
    "arr_data = rdd.collect()\n",
    "file_data = data_file.collect() \n",
    "\n",
    "# now we can print them \n",
    "print(arr_data, file_data, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('emp1', 2000)\n",
      "('emp2', 2500)\n",
      "('emp3', 3000)\n",
      "emp1, 2000\n",
      "emp2, 2500\n",
      "emp3, 3000\n"
     ]
    }
   ],
   "source": [
    "# iterating over items \n",
    "for item in arr_data: \n",
    "    print(item)\n",
    "\n",
    "for item in file_data: \n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emp1 2000\n",
      "emp2 2500\n",
      "emp3 3000\n",
      "emp1  2000\n",
      "emp2  2500\n",
      "emp3  3000\n",
      "[('emp1', ' 2000'), ('emp2', ' 2500'), ('emp3', ' 3000')]\n"
     ]
    }
   ],
   "source": [
    "for e,s in arr_data: \n",
    "    print(e,s)\n",
    "\n",
    "# this is how to preprocess the text into tuples \n",
    "counter = 0\n",
    "for item in file_data: \n",
    "    e,s = item.split(',')\n",
    "    item = (e,s)\n",
    "    file_data[counter] = item\n",
    "    counter += 1 \n",
    "\n",
    "for e,s in file_data: \n",
    "    print(e,s)\n",
    "\n",
    "print(file_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDDs Transformations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[15] at readRDDFromFile at PythonRDD.scala:289\n",
      "[['PySpark', 'is', 'Python', 'API'], ['PySpark', 'supports', 'Spark’s', 'features']]\n"
     ]
    }
   ],
   "source": [
    "# now lets test the map methods. \n",
    "# the .map(), returns a new RDD, which is created by applying a function over each item in the passed RDD.\n",
    "string_data = [(\"PySpark is Python API\"),(\"PySpark supports Spark’s features\")]\n",
    "rdd_string = sc.parallelize(string_data) \n",
    "# rdd_string = rdd_string.collect() \n",
    "print(rdd_string)\n",
    "\n",
    "def splitString (string: str): \n",
    "    return string.split(' ')\n",
    "\n",
    "rdd2 = rdd_string.map(splitString)\n",
    "print(rdd2.collect()) # collect converts the RDD into list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PySpark', 'is', 'Python', 'API', 'PySpark', 'supports', 'Spark’s', 'features']\n"
     ]
    }
   ],
   "source": [
    "# now I want to understand the flatmap \n",
    "rdd3 = rdd_string.flatMap(splitString) # it converts the ND dimensional array into 1d array -> zy el kona bn3mlu fl pc. \n",
    "print(rdd3.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('key1', 4500), ('key2', 3000)]\n"
     ]
    }
   ],
   "source": [
    "# now lets apply the reducebykey method \n",
    "newData = [(\"key1\", 2000), (\"key1\", 2500), (\"key2\", 3000)]\n",
    "newDataRdd = sc.parallelize(newData) # converting them to RDD. \n",
    "reducedRDD = newDataRdd.reduceByKey(lambda x,y: x + y)\n",
    "rddToList = reducedRDD.collect() \n",
    "print(rddToList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PySpark is Python API']\n"
     ]
    }
   ],
   "source": [
    "# we can apply filtering operation \n",
    "filtering = rdd_string.filter(lambda x: 'API' in x)\n",
    "print(filtering.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('PySpark is Python API', 10)]\n",
      "[('PySpark supports Spark’s features', 15)]\n"
     ]
    }
   ],
   "source": [
    "data=[(\"PySpark is Python API\", 10),(\"PySpark supports Spark’s features\", 15)]\n",
    "rdd=sc.parallelize(data)\n",
    "rdd2 = rdd.filter(lambda x : 'API' in x[0]) # gets the first tuple only\n",
    "rdd3 = rdd.filter(lambda x : x[1]>10) # gets the second tuple only. \n",
    "print(rdd2.collect())\n",
    "print(rdd3.collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDDs Actions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "# we use the reduce method to apply some aggregation function on our data \n",
    "# so if we want to get the sum for example, we should do the following \n",
    "data = [1,2,3,4,5] # sum = 5 * 6 / 2 = 15 ;)\n",
    "\n",
    "# convert the list into RDD \n",
    "rdd_data = sc.parallelize(data)\n",
    "\n",
    "# create the logic of sum \n",
    "def sumElm (x1: int, x2: int): \n",
    "    return x1 + x2\n",
    "\n",
    "# apply the reduce \n",
    "sum = rdd_data.reduce(sumElm)\n",
    "\n",
    "# here we do not collect the result. \n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "# get the max \n",
    "def getMax (x1: int, x2: int):\n",
    "    return max(x1, x2)\n",
    "\n",
    "max = rdd_data.reduce(getMax)\n",
    "\n",
    "print(max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 20), (2, 40)]\n",
      "[(2, 40), (1, 20)]\n",
      "[(1, 20), (2, 40)]\n",
      "[(2, 40), (1, 20)]\n"
     ]
    }
   ],
   "source": [
    "# sorting elements \n",
    "unordered=[(1,20),(2,40)]\n",
    "ordered  = sc.parallelize(unordered )\n",
    "#Sort by keys (ascending):\n",
    "print(ordered .takeOrdered(5, key = lambda x: x[0]))\n",
    "\n",
    "#Sort by keys (descending):\n",
    "print(ordered .takeOrdered(5, key = lambda x: -x[0]))\n",
    "\n",
    "#Sort by values (ascending):\n",
    "print(ordered .takeOrdered(5, key = lambda x: x[1]))\n",
    "\n",
    "#Sort by values (descending):\n",
    "print(ordered .takeOrdered(5, key = lambda x: -x[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
